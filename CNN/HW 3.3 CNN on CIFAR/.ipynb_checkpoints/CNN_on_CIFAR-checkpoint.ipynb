{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CE-40959: Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW3. Part 3. CNN on CIFAR (20 + 10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deadline:   16 Farvardin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this problem, we will train a CNN on CIFAR-10 database. Next, we will go through the network and visualize its layers to see what is happening there. Then, we use the trained model in a Transfer Learning approach to classifying the CIFAR-100 database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with loading data. First load the data by runing the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from load_data import load_cifar10 #this function provided in the attached file loads the data of CIFAR10 database\n",
    "\n",
    "def load_cifar10_data():\n",
    "    dataset_dir = 'data/cifar-10-batches-py'\n",
    "    \n",
    "    X_train, y_train, X_test, y_test = load_cifar10(dataset_dir)\n",
    "    X_train, X_val = X_train[:45000], X_train[45000:]\n",
    "    y_train, y_val = y_train[:45000], y_train[45000:]\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, y_train, X_val, y_val, X_test, y_test = load_cifar10_data()\n",
    "\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('X_val shape:', X_val.shape)\n",
    "print('y_val shape:', y_val.shape)\n",
    "print('X_test shape:', X_test.shape)\n",
    "print('y_test shape:', y_test.shape)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(X_train[1])#represnting a sample data from CIFAR-10\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By successfully runing the above code, you have loaded the data and also you can see a sample data from CIFAR-10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code will convert the labels to one-hot codes which will be later used for the classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder # this function is used to prepare one-hot encoded labels\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "\n",
    "encoder.fit(y_train)#this line will fit 'encoder' to encode the labels\n",
    "\n",
    "y_train_onehot = encoder.transform(y_train)# encoding train labels (one-hot coding)\n",
    "y_val_onehot   = encoder.transform(y_val)  # encoding train labels (one-hot coding)\n",
    "y_test_onehot  = encoder.transform(y_test) # encoding test  labels (one-hot coding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below is for converting data to several batches which we will use in training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_batches(x, y, batch_size):\n",
    "    n = len(y)\n",
    "    steps = n // batch_size\n",
    "    if n % batch_size != 0:\n",
    "        steps += 1\n",
    "    x_batches = np.array_split(x, steps)\n",
    "    y_batches = np.array_split(y, steps)\n",
    "    return x_batches, y_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part1: Trainig a CNN (15 pts.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we are going to train a Convolutional Neural Network (CNN) with the below structure:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 16 * conv(5,5), activation function =ReLU\n",
    "2. 16 * conv(5,5), activation function =ReLU\n",
    "3. Max-pooling(3,3), stride=(2,2)\n",
    "4. 32 * conv(5,5), activation function =ReLU\n",
    "5. 32 * conv(5,5), activation function =ReLU\n",
    "6. Max-pooling(3,3), stride=(2,2)\n",
    "7. 64 * conv(5,5), activation function =ReLU\n",
    "8. FC(128), activation function =ReLU\n",
    "9. FC(128), activation function =ReLU\n",
    "10. FC(10), activation function =softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go to implement the model in the below box. Note that using **tf.layers** is **NOT ALLOWED** in this homework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, 32, 32, 3]) # placeholder for input data\n",
    "y = tf.placeholder(tf.float32, [None, 10])# placeholder for labels (one-hot encoded)\n",
    "\n",
    "###################################################################\n",
    "#########  TODO: Your code for implementing the network ###########\n",
    "###################################################################\n",
    "\n",
    "# Define the needed Variables here:\n",
    "# e.g., W_conv1 has the shape of shape=[5, 5, 3, 16] and B_conv1 has the shape of shape=[16]\n",
    "\n",
    "\n",
    "# Define the layers here:\n",
    "conv1 = tf.nn.conv2d(x, W_conv1, strides=[1, 1, 1, 1], padding='SAME') + B_conv1\n",
    "conv1_relu = tf.nn.relu(conv1)\n",
    "conv2 = ...\n",
    "conv2_relu = ...\n",
    "pooling1 = tf.nn.max_pool(conv2_relu, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "# continue the same for other layers:\n",
    "    \n",
    "    \n",
    "    \n",
    "# Don't forget that:\n",
    "# - you need to define variables with needed size and suitable initialization,\n",
    "# - you need to use tf.nn.conv2d to implement convolution,\n",
    "# - you need to use tf.nn.max_pool to implement max_pooling,\n",
    "\n",
    "# Help: your network output is something like: y_hat = tf.nn.softmax(logits) where logits are values of last layer \n",
    "#       before softmax activation function.\n",
    "#############################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you need to define an optimizer. You are free to use any optimizer. You can change optimizer or learning rate if you want :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############ Cross entropy loss ############################\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=y))\n",
    "# Note: logits in the above line is the last layer values (a 10-neuron layer) before the softmax activation function. \n",
    "#       In other words, you may have written y_hat = tf.nn.softmax(logits) in the above box in which y_hat is the output\n",
    "#       of the network.\n",
    "\n",
    "############ Adam optimizer ################################\n",
    "lr = 1e-4 # learning rate\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "trainer = optimizer.minimize(cross_entropy)\n",
    "\n",
    "############################################################\n",
    "#            TODO: Implement Accuracy                      #\n",
    "############################################################\n",
    "correct_preds = ...\n",
    "accuracy = ...\n",
    "# Note: The defined accuray does not play any role in the optimization process. However, it is a mtric which we can \n",
    "#       use as a log for mintoring training process with more metrics rather than a single loss function.\n",
    "############################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can go for the rest of the job, handle the Session and go for training. \n",
    "You can use tensorboard or the code provided in the next box to plot for loss and accuracy of training and validation data.\n",
    "You are expected to achieve at least 60% accuracy on the test data after the training process. Good Luck! ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########## Training ##########\n",
    "batch_size = 64 # You are free to change it\n",
    "x_train_batches, y_train_batches = make_batches(X_train, y_train_onehot, batch_size)\n",
    "epochs = 50  # You are free to change it\n",
    "N_Batches = len(x_train_batches)\n",
    "\n",
    "train_acc  = np.zeros(epochs)\n",
    "train_loss = np.zeros(epochs)\n",
    "val_acc  = np.zeros(epochs)\n",
    "val_loss = np.zeros(epochs)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print()\n",
    "        print(\"*\" * 10, \"Epoch %3i\"%epoch, \"*\" * 10)\n",
    "\n",
    "        batch_acc = np.zeros(N_Batches)\n",
    "        batch_loss = np.zeros(N_Batches)\n",
    "        for n in range(N_Batches):\n",
    "            _, batch_acc[n], batch_loss[n] = sess.run([trainer, accuracy, cross_entropy],\n",
    "                                                      {x: x_train_batches[n], y: y_train_batches[n]})\n",
    "            #print(\"Batch %3d/%d: Acc:%0.4f , Loss:%0.4f\"%(n, N_Batches, batch_acc[n], batch_loss[n]))\n",
    "\n",
    "        train_acc[epoch] = np.mean(batch_acc)\n",
    "        train_loss[epoch] = np.mean(batch_loss)\n",
    "        print(\"Train:\\t\\tAccuracy= %0.4f \\tLoss= %0.4f\"%(train_acc[epoch], train_loss[epoch]))\n",
    "\n",
    "        ########## Validation ##########\n",
    "        val_acc[epoch], val_loss[epoch] = sess.run([accuracy, cross_entropy],\n",
    "                                                   {x: X_val, y: y_val_onehot})\n",
    "        print(\"Validation:\\tAccuracy= %0.4f \\tLoss= %0.4f\"%(val_acc[epoch], val_loss[epoch]))\n",
    "        \n",
    "############################################################\n",
    "#            TODO: Implement Test Phase                    #\n",
    "############################################################\n",
    "...\n",
    "print(\"Test:\\tAccuracy= %0.4f \\tLoss= %0.4f\"%(test_acc, test_loss))\n",
    "############################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Diagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########## Plot metrics #########################\n",
    "fig = plt.figure()\n",
    "plt.plot(range(epochs), train_acc, label='Train')\n",
    "plt.plot(range(epochs), val_acc, label='Validation')\n",
    "plt.title(\"Training and Validation Accuracy\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.grid(True)\n",
    "plt.legend(loc=0)\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.plot(range(epochs), train_loss, label='Train')\n",
    "plt.plot(range(epochs), val_loss, label='Validation')\n",
    "plt.title(\"Training and Validation Loss\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"Cross entropy\")\n",
    "plt.grid(True)\n",
    "plt.legend(loc=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part2: Visualizing the Network (5 pts.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we want to check the weights to see how the network realy works and what it does. Consider the wieghts of the first convolutional layer, find its weights values and print the values of 2 arbitrary filters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##########################################################################\n",
    "# TODO                                                                   #\n",
    "# Note: To retrieve the values of the weight-variables from TensorFlow,  #\n",
    "#       feed-dict is not necessary because                               #\n",
    "#       nothing is calculated.                                           #\n",
    "#       e.g., w = sess.run(weights)                                      #\n",
    "##########################################################################\n",
    "\n",
    "##########################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: Try to interpret what they are doing?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Type your answer here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's visualize the filters of the first conv layer. There are lots of ways to code for visualization. You are free to choose any way you want. However below is function which is provided for you to plot the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def plot_conv_weights(w, input_channel=0):\n",
    "    # The argument w is a 4-dim array.(e.g. weights_conv1 with the shape of [5,5,3,64])\n",
    "    # The argument input_channel shows the specified channel of all filters.\n",
    "    \n",
    "    # Get the lowest and highest values for the weights.\n",
    "    # This is used to correct the colour intensity across\n",
    "    # the images so they can be compared with each other.\n",
    "    w_min = np.min(w)\n",
    "    w_max = np.max(w)\n",
    "    abs_max = max(abs(w_min), abs(w_max))\n",
    "\n",
    "    # Number of filters used in the conv. layer.\n",
    "    num_filters = w.shape[3]\n",
    "\n",
    "    # Number of grids to plot.\n",
    "    # Rounded-up, square-root of the number of filters.\n",
    "    num_grids = math.ceil(math.sqrt(num_filters))\n",
    "    \n",
    "    # Create figure with a grid of sub-plots.\n",
    "    fig, axes = plt.subplots(num_grids, num_grids)\n",
    "\n",
    "    # Plot all the filter-weights.\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        # Only plot the valid filter-weights.\n",
    "        if i<num_filters:\n",
    "            # Get the weights for the i'th filter of the input channel.\n",
    "            # The format of this 4-dim tensor is determined by the\n",
    "            # TensorFlow API. See Tutorial #02 for more details.\n",
    "            img = w[:, :, input_channel, i]\n",
    "\n",
    "            # Plot image.\n",
    "            ax.imshow(img, vmin=-abs_max, vmax=abs_max,\n",
    "                      interpolation='nearest', cmap='seismic')\n",
    "        \n",
    "        # Remove ticks from the plot.\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "    \n",
    "    # Ensure the plot is shown correctly with multiple plots\n",
    "    # in a single Notebook cell.\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "##########################################################################\n",
    "# TODO: call the function above or use your own way here                 #\n",
    "##########################################################################\n",
    "\n",
    "\n",
    "##########################################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: Try to interpret what they are doing?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Type your answer here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part3: Transfer Learning (+10 pts. - optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In all the machine learning problems we need data to train our models. However, the volume of the data might not be sufficient for the considered model. In such cases, we may use a model which is trained on a similar dataset probably for a different objective. Then we transfer to the main problem and fine- tune or replace some parts of the network. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we are going to implement the transfer learning concept on CNN. For this purpose, we use the network that you have trained on Part 1 of this homework (above) to classify three new classes (i.e. bee, elephant, and fox) on CIFAR-100 dataset. Let's first load the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from load_data import load_cifar100 #this function provided in the attached file loads the data of CIFAR100 database\n",
    "\n",
    "def load_cifar100_data():\n",
    "    dataset_dir = 'data/cifar-100-python'\n",
    "    \n",
    "    X_train, y_train, X_test, y_test = load_cifar100(dataset_dir)\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = load_cifar100_data()\n",
    "\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('X_test shape:', X_test.shape)\n",
    "print('y_test shape:', y_test.shape)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(X_train[1])#represnting a sample data from CIFAR-100\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By successfully runing the above code, you have loaded the data and also you can see a sample data from CIFAR-100. The below code will seprate the specified classes (i.e. bee(6), elephant(31), and fox(34)) for the new classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "selected_images_train = np.zeros([1,32,32,3])\n",
    "selected_labels_train = np.zeros([1,1])\n",
    "for i in range(50000):\n",
    "    label = y_train[i]\n",
    "    if label==6 or label==31 or label==34:\n",
    "        selected_images_train = np.append(selected_images_train,np.reshape(X_train[i,:,:,:],[1,32,32,3]),axis=0)\n",
    "        selected_labels_train = np.append(selected_labels_train,np.reshape(label,[1,1]),axis=0)\n",
    "selected_images_train = selected_images_train[1:]# to remove the first defined np..zeros data\n",
    "selected_labels_train = selected_labels_train[1:]# to remove the first defined np..zeros data\n",
    "\n",
    "\n",
    "selected_images_test = np.zeros([1,32,32,3])\n",
    "selected_labels_test = np.zeros([1,1])\n",
    "for i in range(10000):\n",
    "    label = y_test[i]\n",
    "    if label==6 or label==31 or label==34:\n",
    "        selected_images_test = np.append(selected_images_test,np.reshape(X_test[i,:,:,:],[1,32,32,3]),axis=0)\n",
    "        selected_labels_test = np.append(selected_labels_test,np.reshape(label,[1,1]),axis=0)\n",
    "selected_images_test = selected_images_test[1:]# to remove the first defined np..zeros data\n",
    "selected_labels_test = selected_labels_test[1:]# to remove the first defined np..zeros data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "images_train = selected_images_train[:int(selected_images_train.shape[0]*0.8),:,:,:]\n",
    "labels_train = selected_labels_train[:int(selected_images_train.shape[0]*0.8),:]\n",
    "\n",
    "images_validation = selected_images_train[int(selected_images_train.shape[0]*0.8):,:,:,:]\n",
    "labels_validation = selected_labels_train[int(selected_images_train.shape[0]*0.8):,:]\n",
    "\n",
    "images_test = selected_images_test\n",
    "labels_test = selected_labels_test\n",
    "\n",
    "\n",
    "print('images_train shape:', images_train.shape)\n",
    "print('labels_train shape:', labels_train.shape)\n",
    "\n",
    "print('images_validation shape:', images_validation.shape)\n",
    "print('labels_validation shape:', labels_validation.shape)\n",
    "\n",
    "print('images_test shape:', images_test.shape)\n",
    "print('labels_test shape:', labels_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder # this function is used to prepare one-hot encoded labels\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "\n",
    "encoder.fit(selected_labels_train)#this line will fit 'encoder' to encode the labels\n",
    "\n",
    "labels_train_onehot = encoder.transform(labels_train)# encoding train labels (one-hot coding)\n",
    "labels_val_onehot   = encoder.transform(labels_validation)  # encoding train labels (one-hot coding)\n",
    "labels_test_onehot  = encoder.transform(labels_test) # encoding test  labels (one-hot coding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To implement the transfer learning, we need to use the previous model and transfer it to the new problem. For this end, you need to freeze the convolution layers and also the first fully connected layer which means that they should not be updated in the new training process. For the last two fully connected layers, we want to fine-tune the values. However, you should replace the last layer (output layer) with a 3-neuron layer to represent the score for each class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several methods that you can implement the discussed process with TensorFlow. You are free to do any way you want.\n",
    "The below code redefine the network and use the weights of the network trained on CIFAR-10. Then, the frozen weights can be defined as either constant or non-trainable variables. Besides, the fine-tuning weights are also defined as variables with their previous weights in the definition. Anyway, since there are other ways to implement this job, you can do it with your own method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############################################################\n",
    "#            TODO: Implement Accuracy Part                 #\n",
    "############################################################\n",
    "\n",
    "# get the weights form previous network:\n",
    "# e.g., w_conv1_CIFAR10 = sess.run(W_conv1)\n",
    "\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, 32, 32, 3]) # placeholder for input data\n",
    "y_new = tf.placeholder(tf.float32, [None, 3])# placeholder for labels (one-hot encoded)\n",
    "\n",
    "# Re-define the convolutional layers\n",
    "\n",
    "\n",
    "# Define weights for the second FC layer (fc1 is already defined in the previous model)\n",
    "W_fc2_new = \n",
    "B_fc2_new = \n",
    "# Define based on the first fully connected layer.\n",
    "fc2_new = \n",
    "\n",
    "W_fc3_new = \n",
    "B_fc3_new = \n",
    "# Define based on the second fully connected layer (fc2_new).\n",
    "logit = tf.matmul(fc2_new, W_fc3_new) + B_fc3_new\n",
    "\n",
    "y_hat = \n",
    "\n",
    "#############################################################\n",
    "\n",
    "########## Cross entropy loss ##########\n",
    "new_cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logit, labels=y))\n",
    "\n",
    "############################################################\n",
    "#            TODO: Implement the optimizer so that only    #\n",
    "#                  variales defined above are updated.     #\n",
    "############################################################\n",
    "lr = 1e-4\n",
    "new_optimizer = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "new_trainer = new_optimizer.minimize(cross_ent, var_list=[...])\n",
    "############################################################\n",
    "\n",
    "############################################################\n",
    "#            TODO: Implement Accuracy                      #\n",
    "############################################################\n",
    "correct_preds = \n",
    "new_accuracy  = \n",
    "############################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, go for the rest of the job and start training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########## Training ##########\n",
    "batch_size = 64 # You are free to change it\n",
    "x_train_batches, y_train_batches = make_batches(images_train, labels_train_onehot, batch_size)\n",
    "epochs = 30  # You are free to change it\n",
    "N_Batches = len(x_train_batches)\n",
    "\n",
    "train_acc  = np.zeros(epochs)\n",
    "train_loss = np.zeros(epochs)\n",
    "val_acc  = np.zeros(epochs)\n",
    "val_loss = np.zeros(epochs)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch in range(epochs):\n",
    "        print()\n",
    "        print(\"*\" * 10, \"Epoch %3i\"%epoch, \"*\" * 10)\n",
    "\n",
    "        batch_acc = np.zeros(N_Batches)\n",
    "        batch_loss = np.zeros(N_Batches)\n",
    "        for n in range(N_Batches):\n",
    "            _, batch_acc[n], batch_loss[n] = sess.run([new_trainer, new_accuracy, new_cross_entropy],\n",
    "                                                      {x: x_train_batches[n], y: y_train_batches[n]})\n",
    "            #print(\"Batch %3d/%d: Acc:%0.4f , Loss:%0.4f\"%(n, N_Batches, batch_acc[n], batch_loss[n]))\n",
    "\n",
    "        train_acc[epoch] = np.mean(batch_acc)\n",
    "        train_loss[epoch] = np.mean(batch_loss)\n",
    "        print(\"Train:\\t\\tAccuracy= %0.4f \\tLoss= %0.4f\"%(train_acc[epoch], train_loss[epoch]))\n",
    "\n",
    "        ########## Validation ##########\n",
    "        val_acc[epoch], val_loss[epoch] = sess.run([new_accuracy, new_cross_entropy],\n",
    "                                                   {x: images_validation, y: labels_val_onehot})\n",
    "        print(\"Validation:\\tAccuracy= %0.4f \\tLoss= %0.4f\"%(val_acc[epoch], val_loss[epoch]))\n",
    "        \n",
    "############################################################\n",
    "#            TODO: Implement Test Phase                    #\n",
    "############################################################\n",
    "...\n",
    "print(\"Test:\\tAccuracy= %0.4f \\tLoss= %0.4f\"%(test_acc, test_loss))\n",
    "############################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use tensorboard or the code provided in the next box to plot for loss and accuracy of training and validation data. You are expected to achieve at least 80% accuracy on the test data after training. Good Luck! ;)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Diagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########## Plot metrics #########################\n",
    "fig = plt.figure()\n",
    "plt.plot(range(epochs), train_acc, label='Train')\n",
    "plt.plot(range(epochs), val_acc, label='Validation')\n",
    "plt.title(\"Training and Validation Accuracy\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.grid(True)\n",
    "plt.legend(loc=0)\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.plot(range(epochs), train_loss, label='Train')\n",
    "plt.plot(range(epochs), val_loss, label='Validation')\n",
    "plt.title(\"Training and Validation Loss\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"Cross entropy\")\n",
    "plt.grid(True)\n",
    "plt.legend(loc=0)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
