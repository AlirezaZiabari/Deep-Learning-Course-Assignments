{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CE-40959: Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW3. Part 4. CNN Autoencoder (25 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deadline:   16 Farvardin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing modules\n",
    "from utils import load_data\n",
    "from models import AutoEncoder\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (15.0, 15.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# The following two lines let us reload external modules in the notebook\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Description\n",
    "\n",
    "On this notebook, we are going to work on farsi OCR dataset. As its name implies, it is like famous **MNIST** dataset but it consists of images of handwritten digits in farsi. Each instance of this dataset is 32 * 32 gray-scale image. It is totally composed of 80000 instances.\n",
    "Train, test sets are loaded using a method in `utils.py`. Training set includes 0.8 of the whole dataset and the rest is the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Definition\n",
    "The problem we define for this dataset is to reconstruct original image after making some random rotations. We want to develop a model which recieves as input a rotated image and outputs its original without rotation. Meanwhile, a latent embedding is learned in the training process which its quality will be investigated later.\n",
    "\n",
    "First, data loader is called which returns original, rotated and labels for both training and test set. After that from each class on instance is plotted with its rotated counterpart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the whole dataset...\n"
     ]
    }
   ],
   "source": [
    "X_train, X_train_rotated, Y_train, X_test, X_test_rotated, Y_test = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1oAAAIZCAYAAAC/GU7oAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1829cebcc0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# define a subplot of size 2 * number of classes\n",
    "num_class = 10\n",
    "fig, ax = plt.subplots(nrows=2, ncols=num_class)\n",
    "\n",
    "for i in range(num_class):\n",
    "    # gather original images of class i\n",
    "    class_i_images = X_train[Y_train == i]\n",
    "    \n",
    "    # gather rotated images of class i\n",
    "    class_i_rotated = X_train_rotated[Y_train == i]\n",
    "    \n",
    "    # make x and y axis invisible to have a better visualization\n",
    "    ax[0, i].get_xaxis().set_visible(False)\n",
    "    ax[0, i].get_yaxis().set_visible(False)\n",
    "    \n",
    "    # plotting original image of class i\n",
    "    ax[0, i].imshow(np.squeeze(class_i_images[0]))\n",
    "    \n",
    "    # plotting rotated image of class i\n",
    "    ax[1, i].get_xaxis().set_visible(False)\n",
    "    ax[1, i].get_yaxis().set_visible(False)\n",
    "    ax[1, i].imshow(np.squeeze(class_i_rotated[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting of Flags\n",
    "\n",
    "A detailed description about using flags is presented in MLP notebook. So we skip any further explanation here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "flags = tf.app.flags\n",
    "FLAGS = flags.FLAGS\n",
    "flags.DEFINE_string('f', '', 'kernel')\n",
    "flags.DEFINE_float('learning_rate', 0.0005, 'Initial learning rate')\n",
    "flags.DEFINE_integer('width', 32, 'Width of images')\n",
    "flags.DEFINE_integer('height', 32, 'Height of images')\n",
    "flags.DEFINE_integer('num_channel', 1, 'Number of channels of images')\n",
    "flags.DEFINE_integer('batch_size', 10, 'Width of images')\n",
    "flags.DEFINE_integer('num_epochs', 5, 'Number of epochs of training')\n",
    "flags.DEFINE_integer('code_size', 256, 'Size of intermediate embedding')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining Initializers\n",
    "\n",
    "Zero and Noraml intializers are defined in `utils.py`. Based on their explanation, you have to complete their code in `utils.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining Model\n",
    "\n",
    "There is a partly coded class called `AutoEncoder` in `models.py` which has 5 methods for you to complete. In order to develop you model you have to some layers defined in `layers.py` which also require to get completed. \n",
    "\n",
    "Please fill them based on their explanation and then run following cell which defines an AutoEncoder model with **ReLU** as activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-1ed06ac421e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Desktop/Current Semester/Deep Learning/Assignments/DL_HW3_V2/Practical/HW 3.4 CNN Autoencoder/models.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m# self.code_layer: latent code produced in the middle of network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;31m# self.reconstruct: images reconstructed by model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreconstruct\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Current Semester/Deep Learning/Assignments/DL_HW3_V2/Practical/HW 3.4 CNN Autoencoder/models.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    176\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0;31m# evaluate encoding of images by self.encoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m         \u001b[0mcode_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_conv_dims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_rotated_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m         \u001b[0;31m# evaluate reconstructed images by self.decoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Current Semester/Deep Learning/Assignments/DL_HW3_V2/Practical/HW 3.4 CNN Autoencoder/models.py\u001b[0m in \u001b[0;36mencoder\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;31m# convolutional layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mconv1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0;31m# convolutional and pooling layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mconv_pool1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "model = AutoEncoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After building the computation graph of the model, training process should be started by running a session. The following cell runs a session and computes the total loss on each epoch as well.\n",
    "\n",
    "Note that after 5 epochs your loss should get below 1000 (if you have designed everything right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# session\n",
    "sess = tf.Session()\n",
    "\n",
    "# initialize the graph\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# running epochs\n",
    "for epoch in range(FLAGS.num_epochs):\n",
    "    total_loss = 0.0\n",
    "    for batch in range(0, X_train.shape[0], FLAGS.batch_size):\n",
    "        # creating batches\n",
    "        feed_dict = {\n",
    "            model.input_original_images: X_train[batch: batch + FLAGS.batch_size],\n",
    "            model.input_rotated_images: X_train_rotated[batch: batch + FLAGS.batch_size]\n",
    "        }\n",
    "        # feeding batches\n",
    "        _, v_loss = sess.run([model.opt, model.loss], feed_dict=feed_dict)\n",
    "        total_loss += v_loss\n",
    "    print('Epoch {} - Total loss: {}'.format(epoch + 1, total_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the reconstruction result for first batch of test set and compare them with their originals ones as well as their rotated version.\n",
    "\n",
    "In the plot below, rows from 1 to 3 are depicting original, rotated and reconstructed images, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# take a batch from test set\n",
    "feed_dict = {\n",
    "    model.input_original_images: X_test[:FLAGS.batch_size],\n",
    "    model.input_rotated_images: X_test_rotated[:FLAGS.batch_size]\n",
    "}\n",
    "\n",
    "# output reconstructed sample\n",
    "sample_images = sess.run(model.output_images, feed_dict=feed_dict)\n",
    "\n",
    "# rounding pixels of images\n",
    "sample_images[sample_images >= 0.5] = 1\n",
    "sample_images[sample_images < 0.5] = 0\n",
    "\n",
    "# define a subplot of size 3 * batch size\n",
    "fig, ax = plt.subplots(nrows=3, ncols=FLAGS.batch_size, figsize=(18, 3))\n",
    "for i in range(FLAGS.batch_size):\n",
    "    # plot original test image\n",
    "    ax[0, i].get_xaxis().set_visible(False)\n",
    "    ax[0, i].get_yaxis().set_visible(False)\n",
    "    ax[0, i].imshow(np.squeeze(X_test[i]))\n",
    "    \n",
    "    # plot rotated test image\n",
    "    ax[1, i].get_xaxis().set_visible(False)\n",
    "    ax[1, i].get_yaxis().set_visible(False)\n",
    "    ax[1, i].imshow(np.squeeze(X_test_rotated[i]))\n",
    "    \n",
    "    # plot reconstructed image\n",
    "    ax[2, i].get_xaxis().set_visible(False)\n",
    "    ax[2, i].get_yaxis().set_visible(False)\n",
    "    ax[2, i].imshow(np.squeeze(sample_images[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned earlier, let's visualize the embedding produced in the middle of network. By doing so, we can discover how high is the quality of embedding encoded for images.\n",
    "\n",
    "One way to visualize is to use a technique called t-SNE which is basically a dimensionality reduction algorithm. Since the dimension of the latent code is high, it is necessary to use a dimensionality reduction algorithm like t-SNE.\n",
    "\n",
    "There are two options available here to plot t-SNE diagram:\n",
    "1. Using class of `TSNE` defined in scikit-learn libray.\n",
    "2. Tensorboard provides a utility by which embeddings can be visualized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# t-SNE with scikit-learn\n",
    "\n",
    "First, codes generated by the network for some training samples are evaluated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# setting number of samples\n",
    "num_sample = 1000\n",
    "\n",
    "# prepare test data samples\n",
    "feed_dict = {\n",
    "    model.input_original_images: X_train[:num_sample],\n",
    "    model.input_rotated_images: X_train[:num_sample]\n",
    "}\n",
    "labels = Y_train[:num_sample].astype(int)\n",
    "\n",
    "# evaluate embedding on samples\n",
    "embeddings = sess.run(model.code_layer, feed_dict=feed_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A t-SNE class is imported and an object of it constructed. Number of dimensions of new space after reduction is given to constructor. This object has a `fit_transform` method which applies dimension reduction over the input data.\n",
    "\n",
    "In order to have distinguishable points, we set specific color for each 10 classes of digits as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define TSNE and apply it to the input embedding\n",
    "tsne = TSNE(n_components=2)\n",
    "reduced_embeddings = tsne.fit_transform(embeddings)\n",
    "\n",
    "# set colors and labels\n",
    "color_names = ['blue', 'green', 'red', 'orange', 'yellow', 'silver', 'lime', 'black', 'cyan', 'magenta']\n",
    "str_labels = [str(i) for i in range(10)]\n",
    "\n",
    "# list of scatter plot of embedding for each class\n",
    "num_class = 10\n",
    "plts = []\n",
    "for i in range(num_class):\n",
    "    plts.append(plt.scatter(reduced_embeddings[labels == i, 0],\n",
    "                            reduced_embeddings[labels == i, 1]\n",
    "                            , marker='.', c=color_names[i]))\n",
    "\n",
    "# set legend\n",
    "plt.legend(plts, str_labels, \n",
    "           scatterpoints=1, loc='best', \n",
    "           ncol=2, fontsize=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have done everything right, then you have to observe how clusters of different classes are well-separated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# t-SNE with Tensorboard\n",
    "\n",
    "Here, we use tensorboard to visualize latent codes.\n",
    "\n",
    "Generally, we define a `tf.Variable` and assign its value to `code_layer` of our model. Then an object of `ProjectorConfig` is constructed to which a new embedding will be added. This is done by setting the tensor name and the path from which metadata should be taken from. Here metadata corresponds to class labels. Metadata `.csv` file is also creted by `write_metadata` function defined below.\n",
    "\n",
    "Finally a writer is used for saving the embedding and the model is saved by `tf.train.Saver()` to have proper functioning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.contrib.tensorboard.plugins import projector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# metadata writer\n",
    "def write_metadata(labels, path):\n",
    "    with open(os.path.join(path, 'meta_data_labels.csv'), 'w') as csv_file:\n",
    "        for label in labels:\n",
    "            csv_file.write(str(label))\n",
    "            csv_file.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "# set path based on time in logs folder\n",
    "now = datetime.datetime.now()\n",
    "path = './logs/' + str(now.hour) + \"_\" + str(now.minute) + \"_\" + str(now.second) + \"/\"\n",
    "writer = tf.summary.FileWriter(logdir=path, graph=sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# shape of sample tensors\n",
    "tensor_shape = (num_sample, model.code_layer.get_shape()[1].value)\n",
    "embedding_var = tf.Variable(tf.zeros(shape=tensor_shape), name='embedding')\n",
    "\n",
    "# assiging available code_layer in graph to new variable\n",
    "embedding_assign = embedding_var.assign(model.code_layer)\n",
    "\n",
    "# define config\n",
    "config = projector.ProjectorConfig()\n",
    "embedding = config.embeddings.add()  \n",
    "embedding.tensor_name = embedding_var.name  # set tensor name\n",
    "embedding.metadata_path = 'meta_data_labels.csv'  # set metadata path\n",
    "write_metadata(labels, path)  # fill file of metadata\n",
    "projector.visualize_embeddings(writer, config)\n",
    "\n",
    "# save embeddings\n",
    "sess.run(embedding_assign, feed_dict)\n",
    "saver = tf.train.Saver([embedding_var])\n",
    "saver.save(sess, os.path.join(path, 'embedding_layer.ckpt'), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the above code you have to run tensorboard on the path specified in the code.\n",
    "\n",
    "---\n",
    "#### Please have the visualization of tensorboard included in your report.\n",
    "---\n",
    "At the end, do not forget to close the session!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Note**: \n",
    "\n",
    "For this notebook please just include the tensorboard t-SNE visualization of encoded images beside your uploaded files.\n",
    "To do so, you can take screenshot and save it as an image with name `tsne_visual`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
