{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_ferdosi_for_wrod2vec(dir):\n",
    "    sentences = []\n",
    "    with open(dir, 'r') as f:\n",
    "        for b in f:\n",
    "            m1, m2 = b.split(\",\")\n",
    "            split = m1.split(\" \")\n",
    "            sentences.append([elem for elem in split if elem!=''])\n",
    "            split = m2.split(\" \")\n",
    "            split[-1] = split[-1][:len(split[-1])-1]\n",
    "            sentences.append([elem for elem in split if elem!=''])\n",
    "        return np.array(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['به', 'نام', 'خداوند', 'جان', 'و', 'خرد']\n",
      "['کزین', 'برتر', 'اندیشه', 'برنگذرد']\n"
     ]
    }
   ],
   "source": [
    "dataset_dir = '/Users/Alireza/Desktop/Current Semester/Deep Learning/Assignments/DL_HW4/ferdosi.txt'\n",
    "sentences = load_ferdosi_for_wrod2vec(dataset_dir)\n",
    "print (sentences[0])\n",
    "print (sentences[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "window_size = 5\n",
    "hidden_layer = 25\n",
    "epochs = 50\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word2onehot(word, v_count, word_index):\n",
    "    word_vec = np.zeros(v_count)\n",
    "    word_index = word_index[word]\n",
    "    word_vec[word_index] = 1\n",
    "    return word_vec\n",
    "def softmax(x): \n",
    "    e_x = np.exp(x - np.max(x)) \n",
    "    return e_x / e_x.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_training_data(sentences):\n",
    "    word_counts = defaultdict(int)\n",
    "    for row in sentences:\n",
    "        for word in row:\n",
    "            word_counts[word] += 1\n",
    "    v_count = len(word_counts.keys())\n",
    "    words_list = list(word_counts.keys())\n",
    "    word_index = dict((word, i) for i, word in enumerate(words_list))\n",
    "    index_word = dict((i, word) for i, word in enumerate(words_list))\n",
    "\n",
    "    training_data = np.zeros((v_count, v_count))\n",
    "    print (training_data.shape)\n",
    "    for sentence in sentences:\n",
    "        sent_len = len(sentence)\n",
    "        for i, word in enumerate(sentence):\n",
    "            for j in range(max(i - window_size, 0), i + window_size + 1):\n",
    "                if j != i and j <= sent_len-1 and j >= 0 and word_index[word] != word_index[sentence[j]]:\n",
    "                    training_data[word_index[word], word_index[sentence[j]]] += 1\n",
    "    return training_data, words_list, word_index, index_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17658, 17658)\n"
     ]
    }
   ],
   "source": [
    "training_data, words_list, word_index, index_word = generate_training_data(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0. 160.  25. ...   1.   0.   0.]\n",
      "به\n",
      "نام\n"
     ]
    }
   ],
   "source": [
    "print (training_data[0])\n",
    "print (index_word[0])\n",
    "print (index_word[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class word2vec(object): \n",
    "    def __init__(self): \n",
    "        self.words = []\n",
    "        self.V = None\n",
    "        self.word_index = None\n",
    "       \n",
    "    def feed_forward(self,X_index): \n",
    "        self.h = self.W[X_index].reshape(hidden_layer,1)\n",
    "#         print (self.h.shape)\n",
    "        self.u = np.dot(self.W1.T,self.h).reshape(self.V,1)\n",
    "#         print (self.u.shape)\n",
    "        self.y = softmax(self.u).reshape(self.V,1)\n",
    "#         print (self.y.shape)\n",
    "        return self.y \n",
    "           \n",
    "    def backpropagate(self,x_index,t): \n",
    "#         print(\"t shape = \" + str(t.shape))\n",
    "        e = self.y - np.asarray(t).reshape(self.V,1)\n",
    "#         print(\"e shape = \" + str(e.shape))\n",
    "        dLdW1 = np.dot(self.h,e.T) \n",
    "#         print(np.dot(self.W1,e).shape)\n",
    "        dLdW = np.zeros((self.V, hidden_layer)) \n",
    "        dLdW[x_index] = np.dot(self.W1,e).flatten()\n",
    "#         print(dLdW.shape)\n",
    "        self.W1 = self.W1 - learning_rate*dLdW1 \n",
    "        self.W = self.W - learning_rate*dLdW \n",
    "           \n",
    "    def train(self, sentences, epochs): \n",
    "        word_counts = defaultdict(int)\n",
    "        for row in sentences:\n",
    "            for word in row:\n",
    "                word_counts[word] += 1\n",
    "        self.V = len(word_counts.keys())\n",
    "        self.words = list(word_counts.keys())\n",
    "        self.word_index = dict((word, i) for i, word in enumerate(self.words))\n",
    "                \n",
    "        self.W = np.random.uniform(-0.8, 0.8, (self.V, hidden_layer)) \n",
    "        self.W1 = np.random.uniform(-0.8, 0.8, (hidden_layer, self.V))\n",
    "        print (sentences.shape)\n",
    "        for x in range(1,epochs):         \n",
    "            self.loss = 0\n",
    "            sent_len = len(sentence)\n",
    "            for w_target_index, w_context in enumerate(training_data):\n",
    "                self.feed_forward(w_target_index) \n",
    "                self.backpropagate(w_target_index, w_context) \n",
    "                C = 0\n",
    "                for m in range(self.V): \n",
    "                    if(w_context[m]): \n",
    "                        self.loss += -1*self.u[m][0] \n",
    "                        C += 1\n",
    "                self.loss += C*np.log(np.sum(np.exp(self.u))) \n",
    "            print(\"epoch \",x, \" loss = \",self.loss) \n",
    "            learning_rate *= 1/( (1+learning_rate*x) ) \n",
    "              \n",
    "    def predict(self,word,number_of_predictions): \n",
    "        if word in self.words:\n",
    "            X = word2onehot(word, self.V, self.word_index)\n",
    "            prediction = self.feed_forward(X) \n",
    "            output = {} \n",
    "            for i in range(self.V): \n",
    "                output[prediction[i][0]] = i \n",
    "               \n",
    "            top_context_words = [] \n",
    "            for k in sorted(output,reverse=True): \n",
    "                top_context_words.append(self.words[output[k]]) \n",
    "                if(len(top_context_words)>=number_of_predictions): \n",
    "                    break\n",
    "       \n",
    "            return top_context_words \n",
    "        else: \n",
    "            print(\"Word not found in dicitonary\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99218,)\n"
     ]
    }
   ],
   "source": [
    "w2v = word2vec()\n",
    "w2v.train(sentences, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
