{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('ferdosi.txt', encoding='utf-8',header = None)\n",
    "\n",
    "data[0] = data[0].str[:-1]\n",
    "data[1] = data[1].str[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text01 = []\n",
    "for c in data[0]:\n",
    "    \n",
    "    text01 = text01 + list(c)\n",
    "    \n",
    "text02 = []\n",
    "for c in data[0]:\n",
    "    \n",
    "    text02 = text02 + list(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = text01 + text02\n",
    "np.save('text', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = list(np.load('text.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "txt_asci = np.array([ord(c) for c in text]) # Convert to ASCII\n",
    "frq_asci = []\n",
    "\n",
    "for c in range(txt_asci.min(), txt_asci.max()+1):\n",
    "    if (len(np.where(txt_asci == c)[0]) > 0): # Remove infrequent words\n",
    "        frq_asci.append(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chrs = list(chr(x) for x in frq_asci)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chrs = chrs + ['ุค'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numOfchar01 = np.array(list(np.shape(item)[0] for item in list(list(item) for item in data[0])))\n",
    "numOfchar02 = np.array(list(np.shape(item)[0] for item in list(list(item) for item in data[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_char = np.max([np.max(numOfchar02),np.max(numOfchar01)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Dict = dict()\n",
    "for idx,item in enumerate(chrs):\n",
    "    Dict[item] = idx+3\n",
    "Dict['_PAD_'] = 0\n",
    "Dict['_BOM_'] = 1\n",
    "Dict['_EOM_'] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# inv dictionary\n",
    "InvDict = dict()\n",
    "for item in Dict:\n",
    "    InvDict[Dict[item]] = item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mes01 = np.array([[0]*(num_char-len(w))+[1]+[Dict[c] for c in w]+[2] for w in data[0]])\n",
    "mes02 = np.array([[1]+[Dict[c] for c in w]+[2]+[0]*(num_char-len(w)) for w in data[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Train_pre = 0.9\n",
    "\n",
    "all_beyts    = mes01.shape[0]\n",
    "permuted_idx = np.random.permutation(np.arange(all_beyts))\n",
    "num_train    = int(all_beyts*Train_pre)\n",
    "Train_idx    = permuted_idx[:num_train]\n",
    "Test_idx     = permuted_idx[num_train:]\n",
    "\n",
    "Trian_mes01 = mes01[Train_idx,:]\n",
    "Trian_mes02 = mes02[Train_idx,:]\n",
    "Test_mes01 = mes01[Test_idx,:]\n",
    "Test_mes02 = mes02[Test_idx,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# hyper-parameters\n",
    "n_batch = 120\n",
    "hidden_num = 128\n",
    "\n",
    "lr_base = 0.001\n",
    "lr_decay = 0.9999\n",
    "lr_floor = 1e-4\n",
    "\n",
    "n_epoch = 55"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_in_batch = int(Trian_mes01.shape[0]/n_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "####################################################################\n",
    "############################# Train ################################\n",
    "####################################################################\n",
    "\n",
    "\n",
    "# inputs to the model\n",
    "# (None, None) ===> [num_char, batch_size]\n",
    "\n",
    "Encoder_Input = tf.placeholder(dtype = tf.int32, shape=(None, None))\n",
    "Decoder_Input = tf.placeholder(dtype = tf.int32, shape=(None, None))\n",
    "Decoder_Target = tf.placeholder(dtype = tf.int32, shape=(None, None))\n",
    "\n",
    "# change to one hot format\n",
    "\n",
    "EI = tf.one_hot(Encoder_Input, depth = len(chrs), on_value=1.0, off_value=0.0, axis=-1) \n",
    "DI = tf.one_hot(Decoder_Input, depth = len(chrs), on_value=1.0, off_value=0.0, axis=-1) \n",
    "DT = tf.one_hot(Decoder_Target, depth = len(chrs), on_value=1.0, off_value=0.0, axis=-1) \n",
    "\n",
    "#########################################################################\n",
    "############################### Encoder #################################\n",
    "#########################################################################\n",
    "cell_encoder = tf.contrib.rnn.LSTMCell(hidden_num)\n",
    "\n",
    "all_states_ec, final_state_ec = tf.nn.dynamic_rnn(\n",
    "    cell_encoder, EI,\n",
    "    dtype=tf.float32, time_major=True,\n",
    ")\n",
    "\n",
    "del all_states_ec\n",
    "\n",
    "#########################################################################\n",
    "############################### Decoder #################################\n",
    "#########################################################################\n",
    "\n",
    "cell_decoder = tf.contrib.rnn.LSTMCell(hidden_num)\n",
    "\n",
    "all_states_dec, final_state_dec = tf.nn.dynamic_rnn(\n",
    "    cell_decoder, DI,\n",
    "\n",
    "    initial_state=final_state_ec,\n",
    "\n",
    "    dtype=tf.float32, time_major=True, scope=\"plain_decoder\",\n",
    ")\n",
    "\n",
    "# Outputs\n",
    "\n",
    "W = tf.get_variable(name = 'w',shape = (hidden_num, len(chrs)),initializer = tf.contrib.layers.xavier_initializer())\n",
    "b = tf.get_variable(name = 'b',shape = (len(chrs),),initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "logits = tf.einsum('ijk,kl', all_states_dec, W)+ b \n",
    "\n",
    "# loss\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels = DT, logits=logits, dim=-1))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "tf_lr = tf.placeholder(dtype=tf.float32)\n",
    "train_opt = tf.train.AdamOptimizer(learning_rate = tf_lr).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "####################################################################\n",
    "############################# Test #################################\n",
    "####################################################################\n",
    "\n",
    "Encoder_Input_test = tf.placeholder(dtype=tf.int32, shape=(None, None))\n",
    "Decoder_Input_test = tf.placeholder(dtype=tf.int32, shape=(None, None))\n",
    "\n",
    "# Initial_dec_h : activations of hidden layer of LSTM cell\n",
    "# Initial_dec_c : is final output, which can potentially be transfromed with some wrapper\n",
    "Initial_dec_h = tf.placeholder(dtype=tf.float32)\n",
    "Initial_dec_c = tf.placeholder(dtype=tf.float32)\n",
    "\n",
    "initial_state = tuple(\n",
    "  tf.nn.rnn_cell.LSTMStateTuple(Initial_dec_c, Initial_dec_h)\n",
    ")\n",
    "\n",
    "EI_test = tf.one_hot(Encoder_Input_test, depth = len(chrs), on_value=1.0, off_value=0.0, axis=-1) \n",
    "DI_test = tf.one_hot(Decoder_Input_test, depth = len(chrs), on_value=1.0, off_value=0.0, axis=-1)\n",
    "\n",
    "all_states_ec_test, final_state_ec_test = tf.nn.dynamic_rnn(\n",
    "    cell_encoder, EI_test,\n",
    "    dtype=tf.float32, time_major=True,\n",
    ")\n",
    "\n",
    "all_states_dec_test, final_state_dec_test = tf.nn.dynamic_rnn(\n",
    "    cell_decoder, DI_test,\n",
    "\n",
    "    initial_state,\n",
    "\n",
    "    dtype=tf.float32, time_major=True, scope=\"plain_decoder\",\n",
    ")\n",
    "\n",
    "logits_test = tf.einsum('ijk,kl', all_states_dec_test, W) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_loss = []\n",
    "test_loss = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 ======Batch: 0 ======Loss Train: 3.4411871 ======Loss Test 3.4201097\n",
      "Epoch 0 ======Batch: 100 ======Loss Train: 1.8098387 ======Loss Test 2.02748\n",
      "Epoch 1 ======Batch: 0 ======Loss Train: 1.7841098 ======Loss Test 2.0773067\n",
      "Epoch 1 ======Batch: 100 ======Loss Train: 1.6937174 ======Loss Test 2.131368\n",
      "Epoch 2 ======Batch: 0 ======Loss Train: 1.7442652 ======Loss Test 2.0953705\n",
      "Epoch 2 ======Batch: 100 ======Loss Train: 1.6929966 ======Loss Test 2.1448207\n",
      "Epoch 3 ======Batch: 0 ======Loss Train: 1.6622202 ======Loss Test 2.152399\n",
      "Epoch 3 ======Batch: 100 ======Loss Train: 1.5915533 ======Loss Test 2.1980152\n",
      "Epoch 4 ======Batch: 0 ======Loss Train: 1.5928161 ======Loss Test 2.236521\n",
      "Epoch 4 ======Batch: 100 ======Loss Train: 1.5393583 ======Loss Test 2.3356845\n",
      "Epoch 5 ======Batch: 0 ======Loss Train: 1.5376289 ======Loss Test 2.261636\n",
      "Epoch 5 ======Batch: 100 ======Loss Train: 1.5005223 ======Loss Test 2.3948252\n",
      "Epoch 6 ======Batch: 0 ======Loss Train: 1.5079085 ======Loss Test 2.4273944\n",
      "Epoch 6 ======Batch: 100 ======Loss Train: 1.4821848 ======Loss Test 2.4654715\n",
      "Epoch 7 ======Batch: 0 ======Loss Train: 1.4951833 ======Loss Test 2.4759629\n",
      "Epoch 7 ======Batch: 100 ======Loss Train: 1.4801873 ======Loss Test 2.4996932\n",
      "Epoch 8 ======Batch: 0 ======Loss Train: 1.4864742 ======Loss Test 2.4786777\n",
      "Epoch 8 ======Batch: 100 ======Loss Train: 1.4796212 ======Loss Test 2.434942\n",
      "Epoch 9 ======Batch: 0 ======Loss Train: 1.4808729 ======Loss Test 2.4166734\n",
      "Epoch 9 ======Batch: 100 ======Loss Train: 1.4554503 ======Loss Test 2.4666796\n",
      "Epoch 10 ======Batch: 0 ======Loss Train: 1.4801843 ======Loss Test 2.5072052\n",
      "Epoch 10 ======Batch: 100 ======Loss Train: 1.469134 ======Loss Test 2.512093\n",
      "Epoch 11 ======Batch: 0 ======Loss Train: 1.4567093 ======Loss Test 2.4986923\n",
      "Epoch 11 ======Batch: 100 ======Loss Train: 1.449685 ======Loss Test 2.518056\n",
      "Epoch 12 ======Batch: 0 ======Loss Train: 1.4685365 ======Loss Test 2.5434797\n",
      "Epoch 12 ======Batch: 100 ======Loss Train: 1.4545503 ======Loss Test 2.5700686\n",
      "Epoch 13 ======Batch: 0 ======Loss Train: 1.4543889 ======Loss Test 2.552755\n",
      "Epoch 13 ======Batch: 100 ======Loss Train: 1.4509146 ======Loss Test 2.5748277\n",
      "Epoch 14 ======Batch: 0 ======Loss Train: 1.4701934 ======Loss Test 2.5732987\n",
      "Epoch 14 ======Batch: 100 ======Loss Train: 1.4620363 ======Loss Test 2.5639184\n",
      "Epoch 15 ======Batch: 0 ======Loss Train: 1.4736336 ======Loss Test 2.5686557\n",
      "Epoch 15 ======Batch: 100 ======Loss Train: 1.471888 ======Loss Test 2.5717347\n",
      "Epoch 16 ======Batch: 0 ======Loss Train: 1.5024073 ======Loss Test 2.6099725\n",
      "Epoch 16 ======Batch: 100 ======Loss Train: 1.5498735 ======Loss Test 2.455102\n",
      "Epoch 17 ======Batch: 0 ======Loss Train: 1.5146954 ======Loss Test 2.4611492\n",
      "Epoch 17 ======Batch: 100 ======Loss Train: 1.4907982 ======Loss Test 2.524076\n",
      "Epoch 18 ======Batch: 0 ======Loss Train: 1.5040282 ======Loss Test 2.537906\n",
      "Epoch 18 ======Batch: 100 ======Loss Train: 1.4894854 ======Loss Test 2.5652876\n",
      "Epoch 19 ======Batch: 0 ======Loss Train: 1.5002587 ======Loss Test 2.5773294\n",
      "Epoch 19 ======Batch: 100 ======Loss Train: 1.4939618 ======Loss Test 2.5666482\n",
      "Epoch 20 ======Batch: 0 ======Loss Train: 1.5006412 ======Loss Test 2.5570729\n",
      "Epoch 20 ======Batch: 100 ======Loss Train: 1.4935988 ======Loss Test 2.6325486\n",
      "Epoch 21 ======Batch: 0 ======Loss Train: 1.4996297 ======Loss Test 2.6367118\n",
      "Epoch 21 ======Batch: 100 ======Loss Train: 1.4904597 ======Loss Test 2.6432278\n",
      "Epoch 22 ======Batch: 0 ======Loss Train: 1.4850106 ======Loss Test 2.604429\n",
      "Epoch 22 ======Batch: 100 ======Loss Train: 1.4879602 ======Loss Test 2.620668\n",
      "Epoch 23 ======Batch: 0 ======Loss Train: 1.5058999 ======Loss Test 2.6476383\n",
      "Epoch 23 ======Batch: 100 ======Loss Train: 1.499697 ======Loss Test 2.6560059\n",
      "Epoch 24 ======Batch: 0 ======Loss Train: 1.5123913 ======Loss Test 2.6595514\n",
      "Epoch 24 ======Batch: 100 ======Loss Train: 1.5083915 ======Loss Test 2.6816723\n",
      "Epoch 25 ======Batch: 0 ======Loss Train: 1.5100758 ======Loss Test 2.6583133\n",
      "Epoch 25 ======Batch: 100 ======Loss Train: 1.4974207 ======Loss Test 2.6400843\n",
      "Epoch 26 ======Batch: 0 ======Loss Train: 1.5116483 ======Loss Test 2.7109125\n",
      "Epoch 26 ======Batch: 100 ======Loss Train: 1.5102731 ======Loss Test 2.6802096\n",
      "Epoch 27 ======Batch: 0 ======Loss Train: 1.5108749 ======Loss Test 2.6963427\n",
      "Epoch 27 ======Batch: 100 ======Loss Train: 1.5060223 ======Loss Test 2.6513245\n",
      "Epoch 28 ======Batch: 0 ======Loss Train: 1.5046791 ======Loss Test 2.673721\n",
      "Epoch 28 ======Batch: 100 ======Loss Train: 1.5037591 ======Loss Test 2.6651459\n",
      "Epoch 29 ======Batch: 0 ======Loss Train: 1.5202858 ======Loss Test 2.6945045\n",
      "Epoch 29 ======Batch: 100 ======Loss Train: 1.5100183 ======Loss Test 2.6737854\n",
      "Epoch 30 ======Batch: 0 ======Loss Train: 1.5151763 ======Loss Test 2.7043552\n",
      "Epoch 30 ======Batch: 100 ======Loss Train: 1.5136217 ======Loss Test 2.6962109\n",
      "Epoch 31 ======Batch: 0 ======Loss Train: 1.5100452 ======Loss Test 2.7095587\n",
      "Epoch 31 ======Batch: 100 ======Loss Train: 1.5077823 ======Loss Test 2.7114284\n",
      "Epoch 32 ======Batch: 0 ======Loss Train: 1.5134364 ======Loss Test 2.6870363\n",
      "Epoch 32 ======Batch: 100 ======Loss Train: 1.51567 ======Loss Test 2.7291574\n",
      "Epoch 33 ======Batch: 0 ======Loss Train: 1.5348196 ======Loss Test 2.711837\n",
      "Epoch 33 ======Batch: 100 ======Loss Train: 1.5238024 ======Loss Test 2.72263\n",
      "Epoch 34 ======Batch: 0 ======Loss Train: 1.5031041 ======Loss Test 2.64324\n",
      "Epoch 34 ======Batch: 100 ======Loss Train: 1.5086261 ======Loss Test 2.6826668\n",
      "Epoch 35 ======Batch: 0 ======Loss Train: 1.5206589 ======Loss Test 2.7295027\n",
      "Epoch 35 ======Batch: 100 ======Loss Train: 1.5101777 ======Loss Test 2.7224555\n",
      "Epoch 36 ======Batch: 0 ======Loss Train: 1.5134338 ======Loss Test 2.6781614\n",
      "Epoch 36 ======Batch: 100 ======Loss Train: 1.5105569 ======Loss Test 2.661903\n",
      "Epoch 37 ======Batch: 0 ======Loss Train: 1.5266029 ======Loss Test 2.6779456\n",
      "Epoch 37 ======Batch: 100 ======Loss Train: 1.5145334 ======Loss Test 2.7190225\n",
      "Epoch 38 ======Batch: 0 ======Loss Train: 1.5327357 ======Loss Test 2.7021294\n",
      "Epoch 38 ======Batch: 100 ======Loss Train: 1.5325204 ======Loss Test 2.7225802\n",
      "Epoch 39 ======Batch: 0 ======Loss Train: 1.5327942 ======Loss Test 2.7049232\n",
      "Epoch 39 ======Batch: 100 ======Loss Train: 1.5253497 ======Loss Test 2.6874652\n",
      "Epoch 40 ======Batch: 0 ======Loss Train: 1.5337173 ======Loss Test 2.6702151\n",
      "Epoch 40 ======Batch: 100 ======Loss Train: 1.5407012 ======Loss Test 2.6880596\n",
      "Epoch 41 ======Batch: 0 ======Loss Train: 1.5446832 ======Loss Test 2.7008648\n",
      "Epoch 41 ======Batch: 100 ======Loss Train: 1.5419164 ======Loss Test 2.7033753\n",
      "Epoch 42 ======Batch: 0 ======Loss Train: 1.529366 ======Loss Test 2.6678524\n",
      "Epoch 42 ======Batch: 100 ======Loss Train: 1.530765 ======Loss Test 2.6381888\n",
      "Epoch 43 ======Batch: 0 ======Loss Train: 1.55277 ======Loss Test 2.7125015\n",
      "Epoch 43 ======Batch: 100 ======Loss Train: 1.544711 ======Loss Test 2.7185252\n",
      "Epoch 44 ======Batch: 0 ======Loss Train: 1.5592631 ======Loss Test 2.6956718\n",
      "Epoch 44 ======Batch: 100 ======Loss Train: 1.5547478 ======Loss Test 2.7061028\n",
      "Epoch 45 ======Batch: 0 ======Loss Train: 1.558948 ======Loss Test 2.6923857\n",
      "Epoch 45 ======Batch: 100 ======Loss Train: 1.551271 ======Loss Test 2.723295\n",
      "Epoch 46 ======Batch: 0 ======Loss Train: 1.5691321 ======Loss Test 2.754512\n",
      "Epoch 46 ======Batch: 100 ======Loss Train: 1.5662884 ======Loss Test 2.736182\n",
      "Epoch 47 ======Batch: 0 ======Loss Train: 1.5395621 ======Loss Test 2.6911879\n",
      "Epoch 47 ======Batch: 100 ======Loss Train: 1.5424294 ======Loss Test 2.6650338\n",
      "Epoch 48 ======Batch: 0 ======Loss Train: 1.5567052 ======Loss Test 2.6834245\n",
      "Epoch 48 ======Batch: 100 ======Loss Train: 1.5538968 ======Loss Test 2.6756518\n",
      "Epoch 49 ======Batch: 0 ======Loss Train: 1.5593834 ======Loss Test 2.708148\n",
      "Epoch 49 ======Batch: 100 ======Loss Train: 1.5592202 ======Loss Test 2.6751962\n",
      "Epoch 50 ======Batch: 0 ======Loss Train: 1.5561215 ======Loss Test 2.6727297\n",
      "Epoch 50 ======Batch: 100 ======Loss Train: 1.5440784 ======Loss Test 2.687525\n",
      "Epoch 51 ======Batch: 0 ======Loss Train: 1.5578386 ======Loss Test 2.695296\n",
      "Epoch 51 ======Batch: 100 ======Loss Train: 1.566001 ======Loss Test 2.705948\n",
      "Epoch 52 ======Batch: 0 ======Loss Train: 1.5806547 ======Loss Test 2.6859984\n",
      "Epoch 52 ======Batch: 100 ======Loss Train: 1.5689871 ======Loss Test 2.7151933\n",
      "Epoch 53 ======Batch: 0 ======Loss Train: 1.5457611 ======Loss Test 2.6640875\n",
      "Epoch 53 ======Batch: 100 ======Loss Train: 1.5461105 ======Loss Test 2.6507301\n",
      "Epoch 54 ======Batch: 0 ======Loss Train: 1.5716511 ======Loss Test 2.6847453\n",
      "Epoch 54 ======Batch: 100 ======Loss Train: 1.5627955 ======Loss Test 2.7257113\n"
     ]
    }
   ],
   "source": [
    "session = tf.Session()\n",
    "\n",
    "lr = lr_base\n",
    "\n",
    "session.run(tf.global_variables_initializer())\n",
    "\n",
    "for i in range(n_epoch):\n",
    "    for j in range(n_batch):\n",
    "        \n",
    "        # adjusting learning rete \n",
    "        if i > 50:\n",
    "            lr = lr*lr_decay\n",
    "            if lr < lr_floor:\n",
    "                lr = lr_floor\n",
    "        mes1_minibatch = Trian_mes01[(i)*data_in_batch:min((i+1)*data_in_batch,Trian_mes01.shape[0])]\n",
    "        mes2_minibatch = Trian_mes02[(i)*data_in_batch:min((i+1)*data_in_batch,Trian_mes01.shape[0])]\n",
    "                \n",
    "        target = mes2_minibatch\n",
    "        target = np.roll(target,-1,axis=1)\n",
    "        target[0,-1] = 0\n",
    "        \n",
    "        loss_train,_,bela = session.run([loss,train_opt,final_state_ec], feed_dict={Encoder_Input: mes1_minibatch.T,\n",
    "                                                               Decoder_Input: mes2_minibatch.T,\n",
    "                                                               Decoder_Target: target.T,\n",
    "                                                               tf_lr: lr})\n",
    "                \n",
    "        test_idx = np.random.randint(0, Test_mes01.shape[0], size = data_in_batch)\n",
    "        \n",
    "        target_test = Test_mes02[test_idx,:]\n",
    "        target_test = np.roll(target,-1,axis=1)\n",
    "        target_test[0,-1] = 0\n",
    "        \n",
    "        loss_test = session.run(loss,feed_dict={Encoder_Input: Test_mes01[test_idx,:].T,\n",
    "                                                    Decoder_Input: Test_mes02[test_idx,:].T,\n",
    "                                                    Decoder_Target: target_test.T})\n",
    "        \n",
    "        train_loss = train_loss + [loss_train]\n",
    "        test_loss = test_loss + [loss_test]\n",
    "        \n",
    "        if j%100==0:\n",
    "            print(\"Epoch\", i, \"======Batch:\", j, \"======Loss Train:\", loss_train,\"======Loss Test\",loss_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
