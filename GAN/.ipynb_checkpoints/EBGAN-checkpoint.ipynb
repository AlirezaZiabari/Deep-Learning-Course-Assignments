{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-7pwoXYLnZnO"
   },
   "source": [
    "#  CE-40959: Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "11Fphw9kRjq8"
   },
   "source": [
    "## Homework 5 - 2:  EBGAN\n",
    "\n",
    "The goal is to train a GAN with an auto-encoder as its discriminator.\n",
    "For further information read the [paper of EBGAN](https://arxiv.org/abs/1609.03126).\n",
    "\n",
    "Good luck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "Mu_pd9TVFb9v"
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "flZ1MOT8F8kE"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "rg-CYaG9GU03"
   },
   "outputs": [],
   "source": [
    "# MNIST Dataset\n",
    "original_train_dataset = datasets.MNIST(root='./mnist_data/', train=True, transform=transforms.ToTensor(), download=True)\n",
    "original_test_dataset = datasets.MNIST(root='./mnist_data/', train=False, transform=transforms.ToTensor(), download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "wZ06E4yEHbGo"
   },
   "outputs": [],
   "source": [
    "CUDA = True\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "CXUYiAuzGWuh"
   },
   "outputs": [],
   "source": [
    "# Define Train loader\n",
    "train_tensors = original_train_dataset.data.float() / 255\n",
    "test_tensors = original_test_dataset.data.float() / 255\n",
    "\n",
    "train_dataset = torch.utils.data.TensorDataset(train_tensors, original_train_dataset.targets)\n",
    "test_dataset = torch.utils.data.TensorDataset(test_tensors, original_test_dataset.targets)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "os1o4AE-FvGg"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "OUx_-WpBFxRp"
   },
   "outputs": [],
   "source": [
    "def show(image_batch, rows=1):\n",
    "    # Set Plot dimensions\n",
    "    cols = np.ceil(image_batch.shape[0] / rows)\n",
    "    plt.rcParams['figure.figsize'] = (0.0 + cols, 0.0 + rows) # set default size of plots\n",
    "    \n",
    "    for i in range(image_batch.shape[0]):\n",
    "        plt.subplot(rows, cols, i + 1)\n",
    "        plt.imshow(image_batch[i], cmap=\"gray\", vmin=0, vmax=1)\n",
    "        plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JWlDL4yUE8f4"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 91
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2010,
     "status": "ok",
     "timestamp": 1558863155074,
     "user": {
      "displayName": "Alireza Ziabari",
      "photoUrl": "https://lh5.googleusercontent.com/-ciGzsAjhRdY/AAAAAAAAAAI/AAAAAAAAAus/cXIzpbkKXu0/s64/photo.jpg",
      "userId": "18285492427157036835"
     },
     "user_tz": -270
    },
    "id": "b-P5Au4tHZAT",
    "outputId": "287f48e0-4621-4f4d-b050-fba16d1ef260"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "class AutoEncoderMSE(nn.Module):\n",
    "    def __init__(self, input_dim, encoder_dims, decoder_dims, dropout_rate=0.5):\n",
    "        super(AutoEncoderMSE, self).__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.input_dropout = nn.Dropout(p=dropout_rate)      \n",
    "        \n",
    "        # Encoder part\n",
    "        encoder_layers = []\n",
    "        for i in range(len(encoder_dims)):\n",
    "          if i == 0:\n",
    "            encoder_layers.append(nn.Linear(input_dim, encoder_dims[i]))\n",
    "          else:\n",
    "            encoder_layers.append(nn.Linear(encoder_dims[i - 1], encoder_dims[i]))\n",
    "          encoder_layers.append(nn.LeakyReLU(0.2))\n",
    "\n",
    "        self.encoder = nn.Sequential(*encoder_layers)\n",
    "        \n",
    "        last_encoder_dim = ([input_dim] + encoder_dims)[-1]\n",
    "\n",
    "        # Decoder part\n",
    "        decoder_layers = []\n",
    "        for i in range(len(decoder_dims)):\n",
    "          if i == 0:\n",
    "            decoder_layers.append(nn.Linear(encoder_dims[-1], decoder_dims[i]))\n",
    "          else:\n",
    "            decoder_layers.append(nn.Linear(decoder_dims[i - 1], decoder_dims[i]))\n",
    "          decoder_layers.append(nn.LeakyReLU(0.2))\n",
    "        decoder_layers.append(nn.Linear(decoder_dims[-1], input_dim))\n",
    "        \n",
    "        self.decoder = nn.Sequential(*decoder_layers)\n",
    "        \n",
    "        self.MSE = nn.MSELoss(reduction='sum', reduce=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        dropout_output = self.input_dropout(x)\n",
    "        encoder_output = self.encoder(dropout_output)\n",
    "        output = self.decoder(encoder_output)\n",
    "        mse = torch.pow(x - output, 2) / x.shape[1]\n",
    "        return {\"reconstruct\": output, \"loss\": mse}\n",
    "\n",
    "      \n",
    "discriminator = AutoEncoderMSE(784, [256, 128, 64], [128, 256], dropout_rate=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "liKLsKy2H_9a"
   },
   "outputs": [],
   "source": [
    "generator = nn.Sequential(\n",
    "    nn.Linear(128, 128),\n",
    "    nn.LeakyReLU(0.2),\n",
    "    nn.Linear(128, 256),\n",
    "    nn.Dropout(),\n",
    "    nn.LeakyReLU(0.2),\n",
    "    nn.Linear(256, 512),\n",
    "    nn.LeakyReLU(0.2),\n",
    "    nn.Linear(512, 784),\n",
    "    nn.Sigmoid()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "LPNWbLh5JW1L"
   },
   "outputs": [],
   "source": [
    "if CUDA:\n",
    "  discriminator.cuda()\n",
    "  generator.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "Kv2HXF9gIQ-Q"
   },
   "outputs": [],
   "source": [
    "LEARNING_RATE_D = 0.0002\n",
    "LEARNING_RATE_G = 0.0002\n",
    "\n",
    "opt_D = optim.Adam(discriminator.parameters(), lr=LEARNING_RATE_D)\n",
    "opt_G = optim.Adam(generator.parameters(), lr=LEARNING_RATE_G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "7L-wvColI2fW"
   },
   "outputs": [],
   "source": [
    "N_EPOCH = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 21317,
     "output_embedded_package_id": "1W4Cl3eoe7Ze8G2sRFJ7Jcj0g56jdQlFE"
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 826389,
     "status": "ok",
     "timestamp": 1558863979488,
     "user": {
      "displayName": "Alireza Ziabari",
      "photoUrl": "https://lh5.googleusercontent.com/-ciGzsAjhRdY/AAAAAAAAAAI/AAAAAAAAAus/cXIzpbkKXu0/s64/photo.jpg",
      "userId": "18285492427157036835"
     },
     "user_tz": -270
    },
    "id": "T6q_InsGI-ad",
    "outputId": "e0a8f77c-bbab-433b-9b6d-0501bdfe6d32"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Output hidden; open in https://colab.research.google.com to view."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "m = 16\n",
    "\n",
    "for epoch in range(N_EPOCH):\n",
    "    for i, (img, label) in enumerate(train_loader):\n",
    "        img = img.flatten(start_dim=1)\n",
    "\n",
    "        real_img = img\n",
    "        if CUDA:\n",
    "            real_img = real_img.cuda()\n",
    "\n",
    "        z = torch.randn(img.shape[0], 128)\n",
    "        if CUDA:\n",
    "            z = z.cuda()\n",
    "        fake_img = generator(z)\n",
    "\n",
    "        \n",
    "        # Discriminator Part\n",
    "        opt_D.zero_grad()\n",
    "        disc_out = discriminator(real_img)\n",
    "        reconstructed = disc_out[\"reconstruct\"]\n",
    "        fake_tensor = torch.sum(discriminator(fake_img)[\"loss\"], dim=1)\n",
    "        \n",
    "        loss_d = torch.sum(disc_out[\"loss\"]) + torch.sum(torch.max(m - fake_tensor, torch.zeros_like(fake_tensor)))\n",
    "\n",
    "        loss_d.backward(retain_graph=True)\n",
    "        opt_D.step()\n",
    "        \n",
    "        # Generator Part\n",
    "        opt_G.zero_grad()\n",
    "        loss_g = torch.sum(discriminator(fake_img)[\"loss\"])\n",
    "\n",
    "        loss_g.backward()\n",
    "        opt_G.step()\n",
    "        \n",
    "\n",
    "    \n",
    "    print(\"epoch: {} \\t last batch loss D: {} \\t last batch loss G: {}\".format(epoch, loss_d.item(), loss_g.item()))\n",
    "    imgs_to_show = fake_img[:30].view(-1, 28, 28).detach().cpu().numpy()\n",
    "    show(imgs_to_show, rows=3)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "DL_HW5_2_EBGAN.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
